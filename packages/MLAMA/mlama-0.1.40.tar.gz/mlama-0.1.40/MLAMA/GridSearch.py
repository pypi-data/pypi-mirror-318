# -*- coding: utf-8 -*-
"""GridSearch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BVqTLEJnqhQUIeFdt23d1GtBEOgo51AN
"""

from .Wave import Wave
from .Models import stats_data, ARIMA_model, ml_model, gen_ml, arima_res_xgb, check_stationarity
from .Processor import TimeSeriesSplit

# Commented out IPython magic to ensure Python compatibility.
### Import Library
import pandas as pd
import numpy as np
import warnings
warnings.filterwarnings("ignore")
from sklearn.model_selection import GridSearchCV#TimeSeriesSplit,
from sklearn.metrics import mean_squared_error, mean_absolute_error
from math import sqrt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import xgboost as xgb
from xgboost.sklearn import XGBRegressor
from sklearn.ensemble import RandomForestRegressor
# %matplotlib inline
pd.set_option('display.max_rows', None)
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.model_selection import TimeSeriesSplit
from tqdm import tqdm
import datetime
import os.path
from os import path
from pmdarima.arima import auto_arima
import os

import json

#grid search.py
def get_wave_data_dictionary(WAVES, delay_list, delayed_start_matrix, delay_definition_flag):
    """
    Constructs a dictionary containing wave data with various delays.

    Parameters:
        WAVES (list): A list of wave objects, each with a method `get_wave_dates_with_delay`.
        delay_list (list): A list of delay values to apply to each wave.
        delayed_start_matrix (dict): A dictionary mapping wave IDs to lists of delayed start values.
        delay_definition: The definition or object specifying the delay settings for each wave.

    Returns:
        dict: A dictionary with keys in the format 'wave {waveID} delay {delay}', containing data for each wave and delay.
    """
    data_dict = {}

    # Loop through each wave and delay to populate the dictionary
    for wave in WAVES:
        for delay in delay_list:
            # Get wave data, train data, and test data for each delay using get_wave_dates_with_delay
            wave_data, train_data, test_data = wave.get_wave_dates_with_delay(delay, delayed_start_matrix, delay_definition_flag)

            # Construct the key for each combination of wave and delay
            key = f'wave {wave.waveID} delay {delay}'

            # Populate the dictionary with the data for each wave and delay
            data_dict[key] = {
                'all': wave_data,
                'train': train_data,
                'test': test_data
            }

    return data_dict

def run_auto_arima_for_each_key(
    data_dict,
    output_folder,
    tuned_on_filename,
    model_name = 'ARIMA',
    output_filename_template = '{key}_{model_name}_results_{tuned_on_filename}.json'
):
    """
    Run the auto-ARIMA model for each dataset in the provided dictionary
    and save the ARIMA parameters to a JSON file.

    Parameters:
        data_dict (dict): Dictionary containing training and testing data for each dataset.
        output_folder (str): Folder path where the JSON file will be saved.
        tuned_on_filename (str): Filename for the data the models are tuned on.
        output_filename_template (str): Template for the output filename, which should include a placeholder for the dataset key.

    Returns:
        dict: A dictionary containing the ARIMA parameters for each dataset.
    """
    arima_results = {}

    # Save results to JSON
    os.makedirs(output_folder, exist_ok=True)

    # Iterate over each key in the data_dict
    for key, data in tqdm(data_dict.items(), desc="Dataset"):
        print(f"Running auto-ARIMA for key: {key}")

        # Prepare the ARIMA model training data
        arima_train = pd.DataFrame(data['all'].weekcase)

        # Fit the auto-ARIMA model
        model = auto_arima(
            arima_train,
            start_p=1, start_q=1, max_p=10, max_q=10,
            m=52, start_P=0, seasonal=True,
            d=0, D=0, trace=True,
            error_action='ignore',
            suppress_warnings=True,
            stepwise=True
        )

        # Get the ARIMA model parameters
        arima_parameters = model.get_params()
        print(f"ARIMA Parameters for {key}: {arima_parameters}")

        output_filename = output_filename_template.format(key=key,  model_name = model_name, tuned_on_filename = tuned_on_filename)  # Use a template for the filename


        # Store the ARIMA parameters for the current dataset
        arima_results[key] = arima_parameters
        result_path_best = os.path.join(output_folder, 'best_params_'+output_filename)
        with open(result_path_best, "w") as file:
          json.dump(arima_parameters, file)
          print('grid_search_best_params write done.', result_path_best)




        result_path_all = os.path.join(output_folder, output_filename)
        with open(result_path_all, "w") as file:
          json.dump(arima_parameters, file, indent=4)

        print(f"ARIMA parameters saved to: {result_path_all}")


    #arima_results.to_json(result_path_all, orient="records", indent=4)  # Saves as a JSON file

    #grid_search_cv_results.to_csv(result_path_all+'.csv', index=False)
    #grid_search_best_params.to_csv(result_path+'_best_params.csv', index=False)



    return arima_results



def stat_train(data):
  return pd.DataFrame(data['all'].weekcase)

from sklearn.model_selection import GridSearchCV
import os
import json

import joblib

def run_grid_search(
    model,
    cv,
    param_grid,
    lagged_amount,
    key,
    data,
    output_folder,
    output_filename,
    scoring='neg_mean_absolute_percentage_error',
    use_random_state=None,
    custom_steps=None,
    skip_existing_files = True
):
    """
    Generalized function for grid search on any model.

    Parameters:
        model: Machine learning model instance.
        cv: Cross-validation object.
        param_grid: Dictionary of hyperparameters for grid search.
        lagged_amount: Number of lagged points to include in the dataset.
        data_dict: Dictionary containing training and testing data splits (train_X, train_y, test_X, test_y).
        output_folder: Folder to save results.
        output_filename: Filename for saving grid search results.
        scoring: Scoring metric for grid search (default is 'accuracy').
        use_random_state: Optional; Boolean to control whether random state should be used (for specific models).
        custom_steps: Optional; Function for any custom preprocessing or steps specific to the model.

    Returns:
        best_model: Trained model with the best parameters from the grid search.
        best_params: Dictionary of the best hyperparameters.
    """

    #for key, data in tqdm(data_dict.items(), desc='Dataset'):

    # Generate training and testing datasets for ML

    print(data['all'].shape)

    os.makedirs(output_folder, exist_ok=True)
    result_path_all = os.path.join(output_folder, output_filename)
    result_path_best = os.path.join(output_folder, 'best_params_'+output_filename)
    model_filename = os.path.join(output_folder, 'model_'+output_filename)


    # Check if the parameters are already saved
    if skip_existing_files:
         if os.path.exists(result_path_best) and os.path.exists(model_filename) and os.path.exists(result_path_all):
            print(f"{result_path_best}: already DONE")
            with open(result_path_best, "r") as file:
                best_params = json.load(file)
            best_model = joblib.load(model_filename)
            return best_model, best_params

    train_x, test_x, train_y, test_y, train_index, test_index = gen_ml(pd.DataFrame(data['all'].weekcase),1, 1, lagged_amount, [1], resid = False)#gen_ml(pd.DataFrame(data['all'].weekcase), 1, 5, resid=False)

    # Unpack the data dictionary
    #train_X, train_y = data_dict["train_X"], data_dict["train_y"]
    #test_X, test_y = data_dict["test_X"], data_dict["test_y"]

    # Apply any custom preprocessing or steps
    if custom_steps:
        custom_steps(model, train_x, train_y)

    # Adjust parameters for models that need a random state
    if use_random_state is not None and hasattr(model, "random_state"):
        model.random_state = 42 if use_random_state else None

    # Perform grid search
    grid_search = GridSearchCV(
        estimator=model,
        param_grid=param_grid,
        scoring=scoring,
        cv=cv,#5,  # Change as per your requirement
        n_jobs=-1,  # Use all available processors
        error_score='raise',
        verbose=2
    )
    grid_search.fit(train_x, train_y)
    grid_search_cv_results = pd.DataFrame(grid_search.cv_results_)#.to_csv('grid_search_results.csv', index=False)


    # Print JSON string for inspection
    #json_results = grid_search_cv_results.to_json(orient="records", indent=4)  # Pretty-print JSON
    #print(json_results)

    #print(grid_search_cv_results)
    #grid_search_best_params = pd.DataFrame(grid_search.best_params_)
    #grid_search_best_params['key']=key
    #print(grid_search_best_params)

    #print(grid_search_cv_results)

    # Save results to a file

    grid_search_cv_results.to_json(result_path_all, orient="records", indent=4)  # Saves as a JSON file

    #grid_search_cv_results.to_csv(result_path_all+'.csv', index=False)
    #grid_search_best_params.to_csv(result_path+'_best_params.csv', index=False)
    best_params = grid_search.best_params_
    with open(result_path_best, "w") as file:
        json.dump(best_params, file)
        print('grid_search_best_params write done.', result_path_best)
    #with open(result_path, "w") as file:
    #    json.dump(grid_search.cv_results_, file)


    print(f"Grid search results saved to: {result_path_all}")

    # Extract and return the best model and hyperparameters
    best_model = grid_search.best_estimator_
    joblib.dump(best_model, model_filename)
    print(f"Best model saved to: {model_filename}")

    return best_model, best_params

def run_grid_search_for_each_key(
    model_class,
    model_name,
    cv,
    param_grid,
    lagged_amount,
    data_dict,
    output_folder,
    tuned_on_filename,
    output_filename_template = '{key}_{model_name}_results_{tuned_on_filename}.json',
    use_random_state=None,
    custom_steps=None,
    skip_existing_files = True
):
    """
    Generalized function to perform grid search for each key in a data dictionary.

    Parameters:
        model_class: Class of the machine learning model (e.g., XGBClassifier, RandomForestClassifier).
        cv: Cross-validation object (e.g., TimeSeriesSplit).
        param_grid: Dictionary of hyperparameters for grid search.
        lagged_amount: Number of lagged points to include in the dataset.
        data_dict: Dictionary containing data for each key.
        output_folder: Folder to save the grid search results.
        tuned_on_filename: Filename for the data the models are tuned on.
        output_filename_template: Template for the output filename, should include a placeholder for `key`.
        use_random_state: Optional; Boolean to control whether random state should be used (for specific models).
        custom_steps: Optional; Function for any custom preprocessing or steps specific to the model.

    Returns:
        best_models: Dictionary containing the best model for each key.
        best_params: Dictionary containing the best parameters for each key.
    """
    best_models = {}
    best_params = {}

    for key in data_dict.keys():
        print(f"Running grid search for key: {key}")

        best_model, best_params_for_key = run_grid_search(
            model=model_class(),
            cv = cv,
            param_grid=param_grid,
            lagged_amount = lagged_amount,
            key = key,
            data=data_dict[key],  # Process one key at a time
            output_folder=output_folder,
            output_filename=output_filename_template.format(key=key, model_name = model_name, tuned_on_filename = tuned_on_filename),
            use_random_state=use_random_state,
            custom_steps=custom_steps,
            skip_existing_files = skip_existing_files
        )

        best_models[key] = best_model
        best_params[key] = best_params_for_key
        print(f"Key: {key} - Best Model:", best_model)
        print(f"Key: {key} - Best Parameters:", best_params_for_key)

    return best_models, best_params