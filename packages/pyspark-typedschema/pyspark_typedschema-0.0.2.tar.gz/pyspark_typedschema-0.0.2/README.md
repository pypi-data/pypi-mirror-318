# typedschema

This is minimally intrusive library to type or annotate pyspark data frames.

There are existing projects which try to change how you interact with pyspark, but this
is not the goal of this library. Goals:

* Create a simple way to define a schema for a pyspark DataFrame.
* Supply some utility functions to test if the DataFrame adheres to a predefined schema.
* Enable schema column autocompletion in your editor

```python
from datetime import datetime, date
from pyspark.sql.types import (
    DoubleType,
    StringType,
    LongType,
    DateType,
    TimestampType,
)

from typedschema import Column, Schema, diff_schemas

class MySchema(Schema):
    a = Column(LongType(), nullable=False)
    b = Column(DoubleType(), nullable=False)
    c = Column(StringType(), nullable=False)
    d = Column(DateType(), nullable=False)
    e = Column(TimestampType(), nullable=False)

myschema = MySchema()

df1 = spark.createDataFrame(
    [
        (1, 2.0, "string1", date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
        (2, 3.0, "string2", date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
        (3, 4.0, "string3", date(2000, 3, 1), datetime(2000, 1, 3, 12, 0)),
    ],
    schema=myschema.spark_schema,
)
df1.show()

# +---+---+-------+----------+-------------------+
# |  a|  b|      c|         d|                  e|
# +---+---+-------+----------+-------------------+
# |  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|
# |  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|
# |  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|
# +---+---+-------+----------+-------------------+

df1.printSchema()

# root
#  |-- a: long (nullable = true)
#  |-- b: double (nullable = true)
#  |-- c: string (nullable = true)
#  |-- d: date (nullable = true)
#  |-- e: timestamp (nullable = true)

df2 = spark.createDataFrame(
    [
        (1, 2.0, "string1", date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),
        (2, 3.0, "string2", date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),
        (3, 4.0, "string3", date(2000, 3, 1), datetime(2000, 1, 3, 12, 0)),
    ],
    schema="a long, z double, c string, d date, e timestamp"
)
# I can test using Python's set operations
# https://docs.python.org/3/library/stdtypes.html#set
# just make sure that the typed schema is on the left side
myschema >= df2.schema # False due to the missing column "b"
# equal to myschema.issuperset(df2.schema)

for change, my, other in diff_schemas(myschema, df2.schema):
    print(f"{change} {my} {other}")

# - StructField('b', DoubleType(), False) None
# + None StructField('z', DoubleType(), True)
# ! StructField('d', DateType(), False) StructField('d', DateType(), True)
# ! StructField('e', TimestampType(), False) StructField('e', TimestampType(), True)
# ! StructField('a', LongType(), False) StructField('a', LongType(), True)
# ! StructField('c', StringType(), False) StructField('c', StringType(), True)

df1.select(myschema.a).show()
# +---+
# |  a|
# +---+
# |  1|
# |  2|
# |  3|
# +---+

df1.select(F.upper(myschema.a.fcol)).show()
# +--------+
# |upper(a)|
# +--------+
# |       1|
# |       2|
# |       3|
# +--------+

# instead of
# df1.select(F.upper(F.col("a"))).show()
```

## Related Projects

* [GitHub - kaiko-ai/typedspark: Column-wise type annotations for pyspark DataFrames](https://github.com/kaiko-ai/typedspark)
* [GitHub - getyourguide/TypedPyspark: Type-annotate your spark dataframes and validate them](https://github.com/getyourguide/TypedPyspark)
